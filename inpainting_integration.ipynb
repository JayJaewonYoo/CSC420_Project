{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inpainting__videos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY5QQaRmjRE5"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "from scipy import signal\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from google.colab.patches import cv2_imshow # For showing because cv2.imshow doesn't work normally"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX4RkQGioWlb"
      },
      "source": [
        "# Based on:\r\n",
        "# https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/criminisi_cvpr2003.pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTa9x4F_mlLV"
      },
      "source": [
        "class InpaintAlgorithm():\n",
        "    def __init__(self, image, labeled_mask, window_size=9, video=None, curr_frame=0):\n",
        "        assert image.shape[:2] == labeled_mask.shape, 'Image and mask must have same shape in 0 and 1 dimensions (HWC)'\n",
        "\n",
        "        # Original inputs (keeping just in case)\n",
        "        ######################\n",
        "        self.original_image = image.astype(np.float32)\n",
        "        # Image is read in as BGR format, may change to RGB later\n",
        "        self.original_labeled_mask = labeled_mask.astype(np.float32)\n",
        "        mask = np.copy(self.original_labeled_mask).astype(np.uint8)\n",
        "        mask[mask > 0] = 255\n",
        "        (threshold, self.original_mask) = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "        # Mask is binary [0, 255] with 255 inside target, 0 outside\n",
        "        ######################\n",
        "\n",
        "        # Values to be updated (not updated when computing priorities but must be updated after)\n",
        "        ######################\n",
        "        # Note that self.image and self.mask can be updated\n",
        "        self.image = np.copy(self.original_image)\n",
        "        # Image is read in as BGR format, may change to RGB later\n",
        "        self.mask = np.copy(self.original_mask)\n",
        "        self.labeled_mask = np.copy(self.original_labeled_mask)\n",
        "        self.video = np.copy(video)\n",
        "        self.curr_frame = curr_frame\n",
        "        # Mask is binary [0, 255] with 255 inside target, 0 outside\n",
        "        # 1 at edges, 0 otherwise\n",
        "        self.fill_front = cv2.Canny(self.mask, 0, 255) / 255\n",
        "        # 0 inside target, 1 outside target\n",
        "        self.inv_mask = 1 - (np.copy(self.mask) / 255)\n",
        "        self.C = np.copy(self.inv_mask)  # 0 inside target, 1 outside target\n",
        "        ######################\n",
        "\n",
        "        # Constant Scalars\n",
        "        ######################\n",
        "        self.num_frames = 1 # Change later\n",
        "        self.window_size = window_size\n",
        "        # Enforce must be odd and positive\n",
        "        assert self.window_size % 2 != 0 and self.window_size > 0, 'Window size must be odd and positive'\n",
        "        # Default size in paper is 9 \"but in practice require the user to set it to be slightly larger than the largest texture element\"\n",
        "        self.alpha = 255\n",
        "        # self.window_area = np.square(self.window_size)\n",
        "        self.window_k = (self.window_size - 1) // 2  # Half of the window\n",
        "        self.image_height, self.image_width = self.mask.shape\n",
        "        ######################\n",
        "\n",
        "        # Constant Kernels\n",
        "        ######################\n",
        "        self.sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
        "        self.sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
        "        # For calculating normal of target edge\n",
        "        self.simple_grad_kernel_x = np.array([[0, 0, 0], [-1, 0, 1], [0, 0, 0]])\n",
        "        # For calculating normal of target edge\n",
        "        self.simple_grad_kernel_y = np.array([[0, -1, 0], [0, 0, 0], [0, 1, 0]])\n",
        "        # Used for quick confidences calculation\n",
        "        self.ones_window = np.ones((self.window_size, self.window_size))\n",
        "        self.normalization_array = signal.convolve2d(np.ones_like(self.mask), self.ones_window, mode='same', boundary='fill', fillvalue=0)  # Used for quick confidences calculation\n",
        "        ######################\n",
        "\n",
        "        # Arrays calculated using the above variables in the various function\n",
        "        ######################\n",
        "        self.grad_y = None  # Defined in compute_gradients()\n",
        "        self.grad_x = None  # Defined in compute_gradients()\n",
        "        self.edge_normal_y = None  # Defined in compute_normals()\n",
        "        self.edge_normal_x = None  # Defined in compute_normals()\n",
        "        self.data = None  # Defined in compute_data()\n",
        "        self.priorities = None  # Defined in compute_priorities()\n",
        "        self.subimage_source_regions = None # List of subimage source regions (idx corresponds to frame idx)\n",
        "        ######################\n",
        "\n",
        "    # Priority calculation functions\n",
        "    #################\n",
        "    def compute_gradients(self):\n",
        "        # If using lab, have to change this\n",
        "        grayscale_image = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\n",
        "        self.grad_y = ndimage.convolve(grayscale_image, self.sobel_y)\n",
        "        self.grad_x = ndimage.convolve(grayscale_image, self.sobel_x)\n",
        "\n",
        "    def compute_normals(self):\n",
        "        edge_grad_x = ndimage.convolve(self.inv_mask, self.simple_grad_kernel_x)\n",
        "        edge_grad_y = ndimage.convolve(self.inv_mask, self.simple_grad_kernel_y)\n",
        "        self.edge_normal_y = -1 * edge_grad_x\n",
        "        self.edge_normal_x = edge_grad_y\n",
        "        normal_magnitude = np.sqrt(np.square(self.edge_normal_y) + np.square(self.edge_normal_x))\n",
        "        # Prevent divide by 0 by not normalizing these elements\n",
        "        normal_magnitude[normal_magnitude == 0] = 1\n",
        "        self.edge_normal_y /= normal_magnitude\n",
        "        self.edge_normal_x /= normal_magnitude\n",
        "\n",
        "    def compute_data(self):\n",
        "        self.compute_gradients()\n",
        "        self.compute_normals()\n",
        "        data = (self.grad_y * self.edge_normal_y) + (self.grad_x * self.edge_normal_x)\n",
        "        data = np.abs(data)\n",
        "        data[data == 0] = 1e-7\n",
        "        # data *= self.fill_front # self.fill_front is assumed to be 1 at edges, 0 else\n",
        "        data /= self.alpha\n",
        "        self.data = data\n",
        "\n",
        "    def compute_confidences(self):\n",
        "        unnormalized_confidences = signal.convolve2d(self.C, self.ones_window, mode='same', boundary='fill', fillvalue=0)\n",
        "        confidences = unnormalized_confidences / self.normalization_array\n",
        "        # confidences *= self.fill_front # self.fill_front is assumed to be 1 at edges, 0 else\n",
        "        self.C = confidences\n",
        "\n",
        "    def compute_priorities(self):\n",
        "        self.compute_data()\n",
        "        self.compute_confidences()\n",
        "        # self.fill_front is assumed to be 1 at edges, 0 else\n",
        "        self.priorities = self.C * self.data * self.fill_front\n",
        "    #################\n",
        "\n",
        "    # Filling target patch functions\n",
        "    #################\n",
        "    def get_highest_priority_point(self):\n",
        "        # Return point corresponding to highest priority\n",
        "\n",
        "        \"\"\" Return coordinates for pixel with highest priority value \"\"\"\n",
        "        return np.unravel_index(self.priorities.argmax(), self.priorities.shape)\n",
        "\n",
        "    def get_target_patch(self, p):\n",
        "        # Return target patch and relevant information such as in-patch masks and coordinates for patch shape\n",
        "\n",
        "        y, x = p\n",
        "        if y - self.window_k < 0:\n",
        "            y_start_diff = self.window_k - y  # Positive, get y_start = y - y_start_diff\n",
        "        else:\n",
        "            y_start_diff = self.window_k\n",
        "        if x - self.window_k < 0:\n",
        "            x_start_diff = self.window_k - x  # Positive, x_start = x - x_start_diff\n",
        "        else:\n",
        "            x_start_diff = self.window_k\n",
        "        if y + self.window_k + 1 > self.image_height:\n",
        "            # Positive, y_end = y + y_end_diff\n",
        "            y_end_diff = self.image_height - (y + 1)\n",
        "        else:\n",
        "            y_end_diff = self.window_k + 1\n",
        "        if x + self.window_k + 1 > self.image_width:\n",
        "            # Positive, x_end = x + x_end_diff\n",
        "            x_end_diff = self.image_height - (x + 1)\n",
        "        else:\n",
        "            x_end_diff = self.window_k + 1\n",
        "\n",
        "        # y_start, y_end = max(0, y - self.window_k), min(self.image_height, y + self.window_k + 1) # row top, row bottom\n",
        "        # x_start, x_end = max(0, x - self.window_k), min(self.image_width, x + self.window_k + 1) # col left, col right\n",
        "        y_start = y - y_start_diff\n",
        "        x_start = x - x_start_diff\n",
        "        y_end = y + y_end_diff\n",
        "        x_end = x + x_end_diff\n",
        "\n",
        "        rel_coords = (y_start_diff, y_end_diff, x_start_diff, x_end_diff)\n",
        "\n",
        "        target_patch = np.copy(self.image[y_start:y_end, x_start:x_end])\n",
        "\n",
        "        patch_mask = np.copy(self.mask[y_start:y_end, x_start:x_end])\n",
        "        patch_mask[patch_mask == 255] = 1\n",
        "        patch_mask = 1 - patch_mask  # Patch mask is 1 in source, 0 in target\n",
        "\n",
        "        return target_patch, rel_coords, patch_mask\n",
        "\n",
        "    def get_person_idx(self, target_p):\n",
        "        # Finding person_idx to use\n",
        "        person_idx = np.zeros_like(self.mask).astype(np.float32)\n",
        "        person_idx[target_p] = 1\n",
        "        person_idx *= self.labeled_mask\n",
        "        person_idx = np.unique(person_idx)\n",
        "        person_idx = person_idx[np.where(person_idx != 0)]\n",
        "\n",
        "        person_idx = person_idx[0].astype(int)\n",
        "        person_idx -= 1 # Convert to 0-indexing instead of 1-indexing\n",
        "\n",
        "        return person_idx\n",
        "\n",
        "    def define_subimage_sources(self):\n",
        "        # Idea is to provide a set of y, x coordinates to traverse\n",
        "\n",
        "        # Increase size of labeled mask so that fill front is overlapped by it\n",
        "        for label in range(1, np.max(self.labeled_mask).astype(int) + 1):\n",
        "            label_y, label_x = np.where(self.labeled_mask == label)\n",
        "            self.labeled_mask[max(0, label_y.min() - 1):min(self.image_height - 1, label_y.max() + 1), \\\n",
        "                         max(0, label_y.min() - 1):min(self.image_width - 1, label_x.max() + 1)] = label\n",
        "\n",
        "        # labeled_mask = np.copy(self.labeled_mask)\n",
        "\n",
        "        subimage_source_regions = [] # Each idx has coordinates for label idx + 1\n",
        "            # Coordinates are of form [(y_coords_1, x_coords_1), (y_coords_2, x_coords_2), ...]\n",
        "\n",
        "        # Defining subimage source region\n",
        "        for label in range(1, np.max(self.labeled_mask).astype(int) + 1):\n",
        "\n",
        "            # Get height and width of detection\n",
        "            labeled_mask = np.copy(self.labeled_mask)\n",
        "            label_y, label_x = np.where(labeled_mask == label)\n",
        "            box_height = label_y.max() - label_y.min() + 1\n",
        "            box_width = label_x.max() - label_x.min() + 1\n",
        "            \n",
        "            subimage = labeled_mask[max(0, label_y.min() - box_height):min(labeled_mask.shape[0] - 1, label_y.max() + box_height), \n",
        "                                    max(0, label_x.min() - box_width):min(labeled_mask.shape[1] - 1, label_x.max() + box_width)]\n",
        "            \n",
        "            subimage[subimage != label] = -label\n",
        "            \n",
        "            subimage_source_regions.append(np.where(labeled_mask == -label))\n",
        "\n",
        "        self.subimage_source_regions = subimage_source_regions\n",
        "\n",
        "        return\n",
        "\n",
        "    def find_optimal_source(self, target_patch, person_idx, rel_coords, patch_mask):\n",
        "        # Returns exemplar from source region\n",
        "\n",
        "        source_mask = np.copy(self.mask).astype(np.float32)\n",
        "\n",
        "        source_mask[source_mask == 255] = 1\n",
        "        source_fill = np.copy(source_mask).astype(np.float32)\n",
        "        source_mask = 1 - source_mask\n",
        "        # This stacking is probably inefficient and could be replaced\n",
        "        source_mask = np.stack([source_mask] * 3, axis=2)\n",
        "        # source_mask is 0 in target region, 1 in source region\n",
        "        # Use 500 to indicate if it is in target region or not, just an arbitrary number outside of BGR range\n",
        "        source_fill[source_fill == 1] = 500\n",
        "        source_fill = np.stack([source_fill] * 3, axis=2)\n",
        "        # source_fill is 500 in target region, 0 in source_region\n",
        "        source_image = (np.copy(self.image) * source_mask) + source_fill\n",
        "\n",
        "        patch_mask_3d = np.stack([patch_mask] * 3, axis=2)\n",
        "\n",
        "        min_score = best_patch_variance = np.inf\n",
        "        optimal_source_patch = None\n",
        "\n",
        "        y_start_diff, y_end_diff, x_start_diff, x_end_diff = rel_coords\n",
        "\n",
        "        curr_y_source_coords, curr_x_source_coords = self.subimage_source_regions[person_idx]\n",
        "\n",
        "        coord_idxs_to_keep = np.logical_and.reduce((curr_y_source_coords - y_start_diff >= 0, curr_x_source_coords - x_start_diff >= 0, curr_y_source_coords + y_end_diff < self.image_height - 1, curr_x_source_coords + x_end_diff < self.image_width - 1))\n",
        "        curr_y_source_coords = curr_y_source_coords[coord_idxs_to_keep]\n",
        "        curr_x_source_coords = curr_x_source_coords[coord_idxs_to_keep]\n",
        "        \n",
        "        best_i = 0 \n",
        "\n",
        "        for coord_idx in range(len(curr_y_source_coords)):\n",
        "            y = curr_y_source_coords[coord_idx]\n",
        "            x = curr_x_source_coords[coord_idx]\n",
        "\n",
        "            '''\n",
        "            y_start = max(0, y - y_start_diff)\n",
        "            x_start = max(0, x - x_start_diff)\n",
        "            y_end = min(self.image_height - 1, y + y_end_diff)\n",
        "            x_end = min(self.image_width - 1, x + x_end_diff)\n",
        "            '''\n",
        "            y_start = y - y_start_diff\n",
        "            x_start = x - x_start_diff\n",
        "            y_end = y + y_end_diff\n",
        "            x_end = x + x_end_diff\n",
        "\n",
        "            curr_source_patch = source_image[y_start:y_end, x_start:x_end, :]\n",
        "            if 500 in curr_source_patch:  # Invalid patch; not entirely bound in source region\n",
        "                continue\n",
        "            \n",
        "            if self.video is None:\n",
        "                curr_score = np.sum(np.square(curr_source_patch - target_patch) * patch_mask_3d)\n",
        "\n",
        "                patch_mean = np.zeros(3)\n",
        "                patch_mask = np.array(patch_mask, dtype=np.bool)\n",
        "\n",
        "                B = curr_source_patch[:,:,0]\n",
        "                patch_mean[0] = np.mean(B[patch_mask])\n",
        "                \n",
        "                G = curr_source_patch[:,:,1]\n",
        "                patch_mean[1] = np.mean(G[patch_mask])\n",
        "            \n",
        "                R = curr_source_patch[:,:,2]\n",
        "                patch_mean[2] = np.mean(R[patch_mask])\n",
        "\n",
        "                inv_patch_mask = np.invert(patch_mask)\n",
        "                w_mean, w_var = 1, 1\n",
        "                if curr_score <= min_score:\n",
        "                    patch_var = 0 \n",
        "                    patch_var += np.sum(np.square(B[inv_patch_mask] - patch_mean[0]))\n",
        "                    patch_var += np.sum(np.square(G[inv_patch_mask] - patch_mean[1]))\n",
        "                    patch_var += np.sum(np.square(R[inv_patch_mask] - patch_mean[2])) \n",
        "\n",
        "                    if curr_score < w_mean * min_score or patch_var < w_var * best_patch_variance:\n",
        "                        best_patch_variance = patch_var\n",
        "                        min_score = curr_score\n",
        "                        optimal_source_patch = curr_source_patch\n",
        "            else:\n",
        "                frame_window = 3 // 2\n",
        "                start_frame = self.curr_frame - frame_window\n",
        "                end_frame = self.curr_frame + frame_window + 1\n",
        "                \n",
        "                if start_frame < 0:\n",
        "                    diff = abs(start_frame)\n",
        "                    start_frame = 0\n",
        "                    end_frame += diff\n",
        "\n",
        "                video_len = len(self.video)\n",
        "                if end_frame > video_len:\n",
        "                    diff = end_frame - video_len\n",
        "                    start_frame -= diff\n",
        "                    end_frame = video_len\n",
        "                \n",
        "                source_patches = self.video[start_frame:end_frame, y_start:y_end, x_start:x_end]\n",
        "                source_patches[frame_window] = curr_source_patch\n",
        "\n",
        "                for i, source_patch in enumerate(source_patches):\n",
        "                    curr_score = np.sum(np.square(source_patch - target_patch) * patch_mask_3d)\n",
        "\n",
        "                    patch_mean = np.zeros(3)\n",
        "                    patch_mask = np.array(patch_mask, dtype=np.bool)\n",
        "\n",
        "                    B = source_patch[:,:,0]\n",
        "                    patch_mean[0] = np.mean(B[patch_mask])\n",
        "                    \n",
        "                    G = source_patch[:,:,1]\n",
        "                    patch_mean[1] = np.mean(G[patch_mask])\n",
        "                \n",
        "                    R = source_patch[:,:,2]\n",
        "                    patch_mean[2] = np.mean(R[patch_mask])\n",
        "\n",
        "                    inv_patch_mask = np.invert(patch_mask)\n",
        "                    w_mean, w_var = 1, 1\n",
        "                    if curr_score <= min_score:\n",
        "                        patch_var = 0 \n",
        "                        patch_var += np.sum(np.square(B[inv_patch_mask] - patch_mean[0]))\n",
        "                        patch_var += np.sum(np.square(G[inv_patch_mask] - patch_mean[1]))\n",
        "                        patch_var += np.sum(np.square(R[inv_patch_mask] - patch_mean[2])) \n",
        "\n",
        "                        if curr_score < w_mean * min_score or patch_var < w_var * best_patch_variance:\n",
        "                            best_patch_variance = patch_var\n",
        "                            min_score = curr_score\n",
        "                            optimal_source_patch = source_patch\n",
        "                            best_i = i\n",
        "        \n",
        "        assert optimal_source_patch is not None, \"Source region not found.\"\n",
        "        print(\"Optimal Source Patch from Frame: \", self.curr_frame + best_i)\n",
        "        return optimal_source_patch\n",
        "\n",
        "    def update_image_arrays(self, optimal_source_patch, target_p, rel_coords, patch_mask):\n",
        "        # Update all image arrays\n",
        "\n",
        "        y, x = target_p\n",
        "\n",
        "        y_start_diff, y_end_diff, x_start_diff, x_end_diff = rel_coords\n",
        "        y_start = y - y_start_diff\n",
        "        x_start = x - x_start_diff\n",
        "        y_end = y + y_end_diff\n",
        "        x_end = x + x_end_diff\n",
        "\n",
        "        # Updating image\n",
        "        # patch_mask is 0 in target, 1 in source\n",
        "        # Make target area in image 0\n",
        "        # Make source area in patch 0\n",
        "        image_patch_mask = np.stack([patch_mask] * 3, axis=2)\n",
        "        self.image[y_start:y_end, x_start:x_end, :] = \\\n",
        "            (self.image[y_start:y_end, x_start:x_end, :] * image_patch_mask) + \\\n",
        "            (optimal_source_patch * (1 - image_patch_mask))\n",
        "\n",
        "        # Updating mask and related arrays\n",
        "        # Update patch to be source\n",
        "        # mask is 1 at edges, 0 otherwise\n",
        "        # inv_mask is 0 inside target, 1 outside target\n",
        "        self.mask[y_start:y_end, x_start:x_end] = 0\n",
        "        self.fill_front = cv2.Canny(self.mask, 0, 255) / 255\n",
        "        self.inv_mask = 1 - (np.copy(self.mask) / 255)\n",
        "\n",
        "        # Updating C\n",
        "        # Only update in target region\n",
        "        C_origin = self.C[y, x] * np.ones_like(patch_mask)\n",
        "        self.C[y_start:y_end, x_start:x_end] = \\\n",
        "            (self.C[y_start:y_end, x_start:x_end] * patch_mask) + \\\n",
        "            (C_origin * (1 - patch_mask))\n",
        "\n",
        "    def update_image(self):\n",
        "        # Use priorities to update image\n",
        "\n",
        "        target_p = self.get_highest_priority_point()\n",
        "        target_patch, rel_coords, patch_mask = self.get_target_patch(target_p)\n",
        "        person_idx = self.get_person_idx(target_p)\n",
        "        optimal_source_patch = self.find_optimal_source(target_patch, person_idx, rel_coords, patch_mask)\n",
        "        self.update_image_arrays(optimal_source_patch, target_p, rel_coords, patch_mask)\n",
        "\n",
        "    def _do_inpainting_single(self):\n",
        "        # Perform one iteration of inpainting\n",
        "\n",
        "        self.compute_priorities()\n",
        "        self.update_image()\n",
        "\n",
        "    def do_inpainting(self):\n",
        "        # Perform all iterations of inpainting\n",
        "\n",
        "        self.define_subimage_sources()\n",
        "        while np.sum(self.fill_front) > 0:\n",
        "            self._do_inpainting_single()\n",
        "            curr_image = self.image.astype(np.uint8)\n",
        "            \n",
        "            # clear_output(wait=False)\n",
        "        return self.image\n",
        "    #################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsu7S74ypHnh"
      },
      "source": [
        "class InpaintVideo():\n",
        "    def __init__(self, video, mask, window_size=9):\n",
        "        \"\"\"\n",
        "        video: Video input T x H x W x C\n",
        "        mask: Video mask input T x H x W\n",
        "        \"\"\"\n",
        "        assert video.shape[:3] == mask.shape\n",
        "        \n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.original_video = video\n",
        "        self.video = np.copy(self.original_video)\n",
        "\n",
        "        self.original_mask = mask\n",
        "        self.mask = np.copy(self.original_mask)\n",
        "\n",
        "    def inpaint_curr_frame(self, curr_frame):\n",
        "        inpaintAlgorithm = InpaintAlgorithm(self.video[curr_frame], self.mask[curr_frame], \\\n",
        "                                            self.window_size, self.original_video, curr_frame)\n",
        "        inpainted_frame = inpaintAlgorithm.do_inpainting()\n",
        "        \n",
        "        return inpainted_frame\n",
        "    \n",
        "    def inpaint(self):\n",
        "        for frame in range(self.video.shape[0]):\n",
        "            print(\"Frame \", frame)\n",
        "            inpainted_frame = self.inpaint_curr_frame(frame)\n",
        "            cv2_imshow(inpainted_frame)\n",
        "            self.video[frame] = inpainted_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOZpvhPfO4dx"
      },
      "source": [
        "class PedestrianDetector():\r\n",
        "    def __init__(self, cascade_classifier='pedestrian.xml', do_video=False):\r\n",
        "        self.pedestrian_cascade = cv2.CascadeClassifier(cascade_classifier)\r\n",
        "\r\n",
        "        self.do_video = do_video\r\n",
        "\r\n",
        "    def get_frame_masks(self, video_src='pedestrians.avi', video_save_path='display.avi'):\r\n",
        "        # Video Capture object\r\n",
        "        cap = cv2.VideoCapture(video_src)\r\n",
        "\r\n",
        "        # Output video\r\n",
        "        frame_width = int(cap.get(3))\r\n",
        "        frame_height = int(cap.get(4))\r\n",
        "        size = (frame_width, frame_height)\r\n",
        "\r\n",
        "        if self.do_video:\r\n",
        "            video_writer = cv2.VideoWriter(video_save_path,\r\n",
        "                                    cv2.VideoWriter_fourcc(*'MJPG'),\r\n",
        "                                    10, size)\r\n",
        "        else:\r\n",
        "            class EmptyWriter():\r\n",
        "                def write(self, _):\r\n",
        "                    return\r\n",
        "                def release(self):\r\n",
        "                    return\r\n",
        "            \r\n",
        "            video_writer = EmptyWriter()\r\n",
        "\r\n",
        "        masks = []\r\n",
        "        frames = []\r\n",
        "\r\n",
        "        frame_number = 0\r\n",
        "\r\n",
        "        while True:\r\n",
        "            ret, img = cap.read()\r\n",
        "\r\n",
        "            if not ret:\r\n",
        "                break\r\n",
        "\r\n",
        "            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n",
        "            # Get bounding boxes\r\n",
        "            bboxes = self.pedestrian_cascade.detectMultiScale(img_gray, 1.3, 2)\r\n",
        "\r\n",
        "            detections = np.array(bboxes)\r\n",
        "            mask = np.zeros_like(img_gray)\r\n",
        "\r\n",
        "            display = img.copy()\r\n",
        "            for idx, (x, y, w, h) in enumerate(bboxes, 1):\r\n",
        "                cv2.rectangle(display, (x, y), (x + w, y + h), (0, 255, 0), 4)\r\n",
        "                mask[y:y + h, x:x + w] = idx\r\n",
        "\r\n",
        "            frames.append(img)\r\n",
        "            masks.append(mask)\r\n",
        "            video_writer.write(display)\r\n",
        "\r\n",
        "            frame_number += 1\r\n",
        "\r\n",
        "        cap.release()\r\n",
        "        video_writer.release()\r\n",
        "\r\n",
        "        frames = np.stack(frames, axis=0)\r\n",
        "        masks = np.stack(masks, axis=0)\r\n",
        "\r\n",
        "        return frames, masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHnGTIzjPLs0"
      },
      "source": [
        "class Inpainter():\r\n",
        "    def __init__(self, video_path, cascade_classifier='pedestrian.xml', do_video=False):\r\n",
        "        self.person_detector = PedestrianDetector(cascade_classifier, do_video)\r\n",
        "        self.video_frames, self.labeled_masks = self.person_detector.get_frame_masks(video_src=video_path)\r\n",
        "        \r\n",
        "        self.inpaintVideo = InpaintVideo(self.video_frames, self.labeled_masks, window_size=21)\r\n",
        "\r\n",
        "    def inpaint(self):\r\n",
        "        self.inpaintVideo.inpaint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnUzJygYCAf3"
      },
      "source": [
        "person_detector = PedestrianDetector('pedestrian.xml', False)\r\n",
        "video_frames, labeled_masks = person_detector.get_frame_masks('pedestrians.avi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "VQeOrhvVCRDo",
        "outputId": "5dda9848-cdb9-41ca-de6d-04e13489d847"
      },
      "source": [
        "inpaintVideo = InpaintVideo(video_frames, labeled_masks, window_size=21)\r\n",
        "inpaintVideo.inpaint()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frame  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fa10793e4a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minpaintVideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpaintVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minpaintVideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b9e85ce771b0>\u001b[0m in \u001b[0;36minpaint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Frame \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0minpainted_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minpaint_curr_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpainted_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpainted_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b9e85ce771b0>\u001b[0m in \u001b[0;36minpaint_curr_frame\u001b[0;34m(self, curr_frame)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minpaint_curr_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minpaintAlgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpaintAlgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_frame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_frame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0minpainted_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpaintAlgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_inpainting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minpainted_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e273d04037f>\u001b[0m in \u001b[0;36mdo_inpainting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_subimage_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_front\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_inpainting_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mcurr_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e273d04037f>\u001b[0m in \u001b[0;36m_do_inpainting_single\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_inpainting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e273d04037f>\u001b[0m in \u001b[0;36mupdate_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mtarget_patch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mperson_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_person_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0moptimal_source_patch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_patch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperson_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_image_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_source_patch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e273d04037f>\u001b[0m in \u001b[0;36mfind_optimal_source\u001b[0;34m(self, target_patch, person_idx, rel_coords, patch_mask)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_patch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mpatch_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0minv_patch_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3335\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "jR5KRF1Z3uKW",
        "outputId": "f37a709d-5006-472f-9f99-d38a568b370f"
      },
      "source": [
        "test_inpainter = Inpainter('pedestrians.avi', 'pedestrian.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-37d31535ed3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_inpainter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpainter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pedestrians.avi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pedestrian.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Inpainter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "aYjM0xnR54SE",
        "outputId": "7a94cdfd-b8ed-45d5-dba7-67550069ec2b"
      },
      "source": [
        "test_inpainter.inpaint()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n",
            "[151 151 151 ... 230 230 230]\n",
            "[496 497 498 ... 533 534 535]\n",
            "[231 231 231 ... 307 307 307]\n",
            "[239 240 241 ... 288 289 290]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-6f4699f336de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_inpainter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-1730c23ce90e>\u001b[0m in \u001b[0;36minpaint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minpaintVideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-79a6f9d5ba6f>\u001b[0m in \u001b[0;36minpaint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0minpainted_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minpaint_curr_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Frame \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpainted_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-79a6f9d5ba6f>\u001b[0m in \u001b[0;36minpaint_curr_frame\u001b[0;34m(self, curr_frame)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minpaintAlgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpaintAlgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_frame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_frame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0minpainted_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpaintAlgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_inpainting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-76a86d307123>\u001b[0m in \u001b[0;36mdo_inpainting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;31m# Perform all iterations of inpainting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_subimage_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_front\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_inpainting_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-76a86d307123>\u001b[0m in \u001b[0;36mdefine_subimage_sources\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Get height and width of detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mlabel_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mbox_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabel_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mbox_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabel_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     28\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     29\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL9iF2zlCOFA"
      },
      "source": [
        "# For if we want to bilaterally filter outputs\r\n",
        "\r\n",
        "curr_image = cv2.imread('index.png', cv2.IMREAD_COLOR)\r\n",
        "cv2_imshow(cv2.bilateralFilter(curr_image,9,75,75))\r\n",
        "cv2_imshow(curr_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bXN02Am391k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}